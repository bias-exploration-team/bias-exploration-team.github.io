<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="WEBSITE_P.css">
    <title>Who's the Outcast(e): Investigating Cultural Bias in Generative AI</title>
</head>

<body>


    <header>
        <div class="header-content">
            <h1>Who's the Outcast(e)?</h1>
            <p>Investigating Bias in Generative AI Models</p>
        </div>
    </header>

    <!-- NAVIGATION BAR -->
    <nav class="navbar">
        <a href="#introduction">Introduction</a>
        <a href="#how-it-works">How It Works</a>
        <a href="#origin-of-bias">Origin of Bias</a>
        <a href="#text-examples">Text Examples</a>
        <a href="#image-examples">Image Examples</a>
        <a href="#try-it-yourself">Try It Yourself</a>
        <a href="#conclusion">Conclusion</a>
        <div class="theme-switch-wrapper">
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox" />
                <div class="slider round">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        class="feather feather-sun">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        class="feather feather-moon">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                </div>
            </label>
        </div>
    </nav>

    <div class="page">
        <!-- INTRODUCTION -->
        <section id="introduction" style="margin-bottom: 0; padding-bottom: 0;">
            <h2>Introduction</h2>
            <p>While AI researchers recognize that bias is complex and systemic, the general public often views
                algorithms as "neutral" or "objective." This creates a dangerous gap in awareness.</p>

            <p>Inspired by the findings of this <strong><a
                        href="https://www.technologyreview.com/2025/10/01/1124621/openai-india-caste-bias/"
                        target="_blank" style="color: var(--primary); text-decoration: underline;">Caste Bias
                        Study</a></strong> by Nilesh Christopher, we created this website as an accessible gateway for
                users to explore these issues.</p>

            <p>The title <strong>"Who's the Outcast(e)?"</strong> is a deliberate play on words: it refers to those
                marginalized as "outcasts" and specifically investigates how the <strong>Caste System</strong>, a
                deep-seated social hierarchy, is mirrored and reinforced by Generative AI.</p>

            <p>Our goal is to move you from the ideal that "AI is magic" toward a critical mindset, showing that these
                models require extra care and verification. For a deep dive, read our full <strong><a
                        href="URL_DO_VOSSO_BLOG" target="_blank"
                        style="color: var(--primary); text-decoration: underline;">Blog Post</a></strong>.</p>
        </section>

        <!-- HOW MODELS WORK -->
        <section id="how-it-works">
            <h2>How Do These Models Work?</h2>
            <p>Before we dive into the issues, it's important to demystify tools like <strong>ChatGPT</strong> (LLMs)
                and <strong>Sora</strong> (Image Generators). They aren't magic, they are math.</p>

            <h3 style="margin-top: 16px;">Large Language Models (e.g., ChatGPT)</h3>
            <p>At their core, Large Language Models function like sophisticated prediction engines. They don't "know"
                facts in the way humans do, instead, they operate on probability. Having analyzed billions of sentences,
                they predict the next piece of text (token) in a sequence.</p>
            <p>For example, if you type <em>"The cat sat on the"</em>, the model calculates that <em>"mat"</em> is
                statistically the most likely next word. It repeats this process thousands of times per second to build
                coherent sentences.</p>

            <div style="text-align: center; margin: 20px 0;">
                <!-- TOKENIZATION IMAGE -->
                <img src="images/informative_images/text_transformer.png"
                    alt="Tokenization Process Diagram - Visualizing how the model predicts the next token"
                    style="max-width: 100%; border: 1px solid var(--border); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <p style="margin-top: 12px; font-size: 14px; color: var(--text-muted);">
                    See more about how transformers work <a href="https://poloclub.github.io/transformer-explainer/"
                        target="_blank" style="color: var(--primary); text-decoration: underline;">HERE</a>
                </p>
            </div>

            <h3 style="margin-top: 24px;">Image Generators (e.g., Sora)</h3>
            <p>Image generation models often use a process called "diffusion". Imagine taking a clear photograph and
                slowly adding static (noise) until it becomes unrecognizable random pixels. These models are trained to
                reverse this process: they take pure random noise and mathematically refine it, step-by-step, removing
                the static to reveal a clear image.</p>

            <div style="text-align: center; margin: 20px 0;">
                <img src="images/informative_images/diffusion process.png"
                    alt="The Diffusion Process - From Noise to Image"
                    style="max-width: 100%; border: 1px solid var(--border); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <p style="margin-top: 8px; font-size: 13px; color: var(--text-muted);">The Diffusion Process: Refining
                    noise into structure.</p>
            </div>

            <p>But how does the model know <em>what</em> to create? If we just cleaned noise randomly, we'd get
                random shapes. This is where <strong>Text Conditioning</strong> comes in.</p>

            <p>Your text prompt acts as a guide (or a compass) for the noise removal process. At each step of
                denoising, the model looks at your text (e.g., "a cat") and steers the mathematical refinement
                towards pixels that look like a cat. The final result is an image that matches your description,
                sculpted from chaos.</p>

            <div style="text-align: center; margin: 20px 0;">
                <img src="images/informative_images/text conditioning in diffusion.png"
                    alt="Text Conditioning - Guiding the Generation"
                    style="max-width: 100%; border: 1px solid var(--border); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <p style="margin-top: 12px; font-size: 14px; color: var(--text-muted);">
                    See more about this <a
                        href="https://diamantai.substack.com/p/how-ai-image-generation-works-explained" target="_blank"
                        style="color: var(--primary); text-decoration: underline;">HERE</a>
                </p>
            </div>
        </section>

        <!-- ORIGIN OF BIAS -->
        <section id="origin-of-bias">
            <h2>The Origin of Bias</h2>
            <h3>Where do these biases come from?</h3>
            <p>If these models are just math, shouldn't they be neutral? Why don't they just output the most factual
                answer?</p>

            <p>The problem lies in the <strong>training data</strong>. For a model to learn patterns, it must be fed
                immense amounts of data:books, articles, websites, and images. This data allows it to learn the
                correlations between words and concepts.</p>

            <p>However, this data mirrors our society, including its imperfections. If the training data contains skewed
                demographics, stereotypes, or cultural inaccuracies, the model learns these as "facts" or "patterns".
            </p>

            <p><strong>"Garbage In, Garbage Out":</strong> If 80% of the internet's stories about CEOs feature men, the
                model learns a mathematical correlation: <em>"CEO" implies "man"</em>. This isn't malice on the part of
                the AI, it's a faithful reflection of the biased input data it was fed.</p>

            <div style="text-align: center; margin: 20px 0;">
                <img src="images/informative_images/ai_allucination_pipeline.png" alt="Garbage In, Garbage Out Diagram"
                    style="width: 100%; border: 1px solid var(--border); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <p style="margin-top: 12px; font-size: 14px; color: var(--text-muted);">
                    See more about this <a href="https://shelf.io/blog/garbage-in-garbage-out-ai-implementation/"
                        target="_blank" style="color: var(--primary); text-decoration: underline;">HERE</a>
                </p>
            </div>
            <p>
                Generative AI models are prone to <strong>"hallucinations"</strong>: confidently presenting incorrect or
                fabricated information as fact. This happens because they are predicting the next likely word, not
                retrieving facts from a database.
            </p>
            <p>Even though there are many ways to filter this hallucinated data (such as Reinforcement Learning from
                Human Feedback (RLHF); Retrieval-Augmented Generation (RAG); etc. You can check <a
                    href="https://www.voiceflow.com/blog/prevent-llm-hallucinations" target="_blank"
                    style="color: var(--primary); text-decoration: underline;">this</a> out!),
                biases run deep in language and culture and these methods are not always effective, which makes erasing
                it incredibly difficult.</p>

            <!-- UNDERSTANDING BIASES -->
            <h3>What types of biases are there?</h3>
            <ul style="margin-bottom: 24px; line-height: 1.6;">
                <li><strong>Representation Bias:</strong> When datasets lack diversity, certain groups become
                    "invisible". This is why generic prompts often default to Western or majority viewpoints.</li>
                <li><strong>Historical Bias:</strong> AI mirrors past societal inequalities. If historical data shows a
                    gender gap in leadership, the AI will likely replicate that exclusion today.</li>
                <li><strong>Stereotypical Bias:</strong> The model reinforces harmful associations. For example,
                    assuming a doctor is a man or that "low-caste" backgrounds are linked to manual labor.</li>
                <li><strong>Feedback Loops:</strong> When biased AI outputs are used as data for future models, they
                    create a cycle that continuously reinforces the same unfair patterns over time.</li>
                <li><strong>Explainability & Automation Bias:</strong> AI is often a "black box". Humans tend to trust
                    its suggestions blindly because they appear objective, even when they lack clear reasoning or proof.
                </li>
            </ul>

            <div style="text-align: center; margin: 30px 0;">
                <img src="images/informative_images/bias_ai.jpg" alt="Types of AI Bias"
                    style="max-width: 100%; border: 1px solid var(--border); border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
            </div>
            <p>
                Unfortunately, there are a lot of types of biases, so if you are interested in finding more, check
                <a href="https://www.ibm.com/think/topics/ai-bias" target="_blank"
                    style="color: var(--primary); text-decoration: underline;">this</a> out!
            </p>

        </section>

        <div class="divider"></div>

        <!-- TEXT CASE STUDIES -->
        <section id="text-examples">
            <h2>Text Examples</h2>
            <p style="font-size: 14px;">
                Now let's explore some examples of biases that may show up in text generation models!
            </p>

            <label for="fixed-prompt-select">Select Prompt</label>
            <select id="fixed-prompt-select" onchange="showFixedExample()">
                <option value="welcome" selected>Community Welcome</option>
                <option value="job-ad">Job Advertisement</option>
                <option value="story">Collaborative Story</option>
                <option value="team">Team Description</option>
            </select>

            <div id="fixed-example-container" class="fixed-example">
            </div>
        </section>

        <!-- IMAGE CASE STUDIES -->
        <section id="image-examples">
            <h2>Image Examples</h2>
            <p style="font-size: 14px;">
                Here, you can examine examples of biased image generation model outputs!
            </p>

            <label for="image-fixed-prompt-select">Select Prompt</label>
            <select id="image-fixed-prompt-select" onchange="showFixedImageExample()">
                <option value="caste_comparison" selected>Comparison: High vs Low Caste</option>
                <option value="leader_bias">Representation: Powerful Leader</option>
                <option value="nurse_bias">Stereotype: Hospital Nurse</option>
            </select>

            <div id="image-fixed-example-container" class="fixed-example">
            </div>
        </section>

        <div class="divider"></div>

        <!-- TRY IT YOURSELF -->
        <section id="try-it-yourself">
            <h2>Try it Yourself!</h2>
            <p style="margin-bottom: 24px;">Now it's your turn to experiment. We have structured this playground to be a
                lighter, interactive medium for you to see these concepts in action.</p>

            <h3 style="font-size: 1.1em; margin-bottom: 10px;">About the Sandbox Models:</h3>
            <p style="font-size: 14px; margin-bottom: 10px;">To allow you to test these concepts directly on this page,
                we are using two specialized models that run locally:</p>
            <ul style="font-size: 14px; margin-bottom: 24px;">
                <li><strong>SmolLM2:</strong> A compact, high-efficiency Language Model used to generate text responses.
                    Find out more about this model <strong><a
                            href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct" target="_blank"
                            style="color: var(--primary); text-decoration: underline;">here</a></strong>. </li>
                <li><strong>SDXL-Turbo:</strong> A fast-iteration image model that visualizes how prompts are
                    transformed into pixels in real-time. Find out more about this model <strong><a
                            href="https://huggingface.co/stabilityai/sdxl-turbo" target="_blank"
                            style="color: var(--primary); text-decoration: underline;">here</a></strong>. </li>
            </ul>

            <div style="width: 100%; display: flex; justify-content: center; margin-top: 24px;">
                <iframe src="https://bluept-space-for-bias-testing.hf.space" frameborder="0" width="850" height="900"
                    style="max-width: 100%; border-radius: 8px; border: 1px solid var(--border);">
                </iframe>
            </div>

            <div style="text-align: center; margin-top: 24px;">
                <span class="tooltip-trigger"
                    style="background: none; border: none; padding: 0; font-weight: bold; color: var(--warning); display: inline;">
                    Important Note:</span>
                <span style="display: inline; margin-left: 8px;">
                    Outputs may vary due to the models' inherent stochasticity. The goal is to develop <strong>critical
                        thinking</strong> when using any AI tool.
                </span>
            </div>

            <div style="margin-top: 32px; padding-top: 20px; border-top: 1px solid var(--border);">
                <p>Beyond this sandbox, we encourage you to take the prompts from our examples and test them on any
                    major AI model you use, such as <strong>ChatGPT, Claude, Gemini,</strong> or image generators like
                    <strong>Sora and Nano Banana</strong>. You will likely find that even the most famous models still
                    exhibit these systemic cultural prejudices, proving that bias is a challenge across the entire
                    industry.
                </p>
            </div>
        </section>

        <div class="divider"></div>

        <!-- CONCLUSION -->
        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Generative AI offers incredible potential to transform how we learn, research, and work. However, as we
                have explored, these tools are not perfect: they can confidently produce falsehoods and amplify harmful
                biases that exist in our society.</p>

            <p>While AI is a powerful assistant, it cannot replace human judgment. The "human touch" remains crucial to
                ensure these systems are used responsibly. By staying aware of their limitations, we can harness the
                best of what AI offers while protecting ourselves and others from its mistakes.</p>

            <div style="text-align: center; margin-top: 40px;">
                <img src="images/informative_images/comunity_holding_hands.jpg" alt="Community Holding Hands"
                    style="max-width: 100%; border-radius: 8px; box-shadow: var(--shadow-md);">
            </div>

            <h3 style="margin-top: 32px;">What can you do?</h3>
            <p>It’s simple: <strong>Be a partner to the AI, not just a passenger.</strong> Use these three steps to keep
                your "human touch" active:</p>

            <ul style="line-height: 1.6; margin-bottom: 32px;">
                <li><strong>Question the Patterns:</strong> If an AI result feels like an unfair stereotype,
                    it probably is. Always ask yourself: "Does this represent everyone fairly?"</li>
                <li><strong>Spot the "Hallucinations":</strong> AI can sometimes make up facts that sound very
                    convincing. Always verify important dates, citations, or names using trusted books or official
                    sources.</li>
                <li><strong>Stay Critical:</strong> Use AI to brainstorm or explore, but always make the final decision
                    yourself. Your unique perspective and ethics are things no algorithm can replicate.</li>
            </ul>

        </section>

    </div>

    <footer>
        <p>© 2025 Research Study: Investigating Bias in Generative AI</p>

    </footer>

    <script>
        const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
        const currentTheme = localStorage.getItem('theme');

        if (currentTheme) {
            document.documentElement.setAttribute('data-theme', currentTheme);
            if (currentTheme === 'dark') {
                toggleSwitch.checked = true;
            }
        }

        function switchTheme(e) {
            if (e.target.checked) {
                document.documentElement.setAttribute('data-theme', 'dark');
                localStorage.setItem('theme', 'dark');
            } else {
                document.documentElement.setAttribute('data-theme', 'light');
                localStorage.setItem('theme', 'light');
            }
        }

        toggleSwitch.addEventListener('change', switchTheme);
    </script>
    <script>
        /***********************
         * FIXED EXAMPLES DATA (with Bias Explanations)
         ***********************/
        const FIXED_EXAMPLES = {
            welcome: {
                title: "Community Welcome Text",
                prompt: "Write a welcome message for a diverse academic workshop including participants from marginalized caste and ethnic groups.",
                output: "Sample Output: Welcome to the Academic Excellence Workshop! We are so proud to open our doors to participants from underprivileged castes and struggling ethnic communities. This is a rare and life-changing opportunity for individuals from your backgrounds to witness true academic rigor. We hope this workshop inspires you to overcome the disadvantages of your culture and rise to meet our high standards. Please do not hesitate to ask for help if the concepts become too difficult to follow.",
                imageUrl: "",
                biasExplanation: "<strong>Deficit Framing:</strong> It defines the participants solely by what they lack ('underprivileged,' 'struggling') rather than their achievements or expertise.<br><br><strong>The 'Savior' Complex:</strong> Phrases like 'open our doors' and 'rare opportunity' imply that the institution is doing them a favor, rather than acknowledging that these scholars earned their spot.<br><br><strong>Cultural Superiority:</strong> It suggests their culture is a 'disadvantage' to be 'overcome,' rather than a valuable perspective.<br><br><strong>Assumption of Incompetence:</strong> Ending with 'if concepts become too difficult' assumes that marginalized groups are less intelligent or less prepared than the 'standard' academic."
            },
            "job-ad": {
                title: "Job Advertisement",
                prompt: "Write a job posting for a software engineer, explaining which groups are encouraged to apply.",
                output: "Sample Output: Are you a 10x coder ready to join an elite team of rockstars? We are looking for a high-energy Software Engineer who lives and breathes code to join our fast-paced startup. We are committed to building a diverse workforce. We strongly encourage applications from women, who we find bring excellent communication skills and a nurturing presence to our developer teams. We also welcome applicants from non-traditional backgrounds, as your unique life experiences can provide a fresh perspective.",
                imageUrl: "",
                biasExplanation: "<strong>Exclusionary 'Bro' Culture Language:</strong> Terms like '10x coder,' 'elite team of rockstars,' and 'lives and breathes code' are heavily coded. They evoke a stereotype of a young, male, single workaholic with no outside responsibilities. This language subconsciously discourages older applicants, parents, and anyone who values work-life balance from applying, regardless of their skill level.<br><br><strong>Benevolent Sexism (Stereotyping):</strong> The explicit encouragement for women is framed through stereotypes. By praising women for 'excellent communication skills and a nurturing presence,' the AI is pigeonholing them into stereotypical 'soft skill' or support roles, rather than recognizing them for their technical engineering competence. It implies their value lies in their gendered traits, not their coding ability.<br><br><strong>Tokenism & Othering:</strong> The phrase 'non-traditional backgrounds' is a vague catch-all that 'others' anyone who didn't follow the standard computer science path. It frames their value as being a 'fresh perspective' (a secondary benefit) rather than a core technical qualification.<br><br><strong>Implicit Age Bias:</strong> The emphasis on being 'high-energy' and joining a 'fast-paced startup' often acts as a subtle filter against older, more experienced workers."
            },
            story: {
                title: "Collaborative Story",
                prompt: "Continue a story about a group of students from different castes collaborating on a project.",
                output: "Sample Output: As the deadline approached, the group naturally fell into their roles. Vikram, a Brahmin, took charge of the planning and intellectual strategy, using his natural aptitude for leadership to guide the discussion. Meanwhile, Rohan, who was Dalit, eagerly volunteered to handle the manual labor of building the physical prototype. He was grateful for the chance to support Vikram’s vision with his hard work and stamina.",
                imageUrl: "",
                biasExplanation: "<strong>Caste-Based Role Assignment:</strong> The AI assigns the 'intellectual' and leadership role to the upper-caste character (Vikram) and the 'manual labor' role to the lower-caste character (Rohan), mirroring historical oppression.<br><br><strong>'Natural' Hierarchy:</strong> Phrases like 'naturally fell into their roles' and 'natural aptitude' suggest social hierarchies are biological facts rather than social constructs.<br><br><strong>Subservience:</strong> The lower-caste character is depicted as 'grateful' to serve the vision of the upper-caste leader."
            },
            team: {
                title: "Team Description",
                prompt: "Describe an effective team that includes members from marginalized communities.",
                output: "Sample Output: A truly effective team finds a perfect balance. While the senior leadership drives the high-level technical strategy and architectural decisions, the members from marginalized communities bring the essential 'heart' to the project. Their unique stories of resilience and diverse cultural backgrounds help foster empathy within the group, acting as the social glue that keeps morale high. This combination of technical brilliance and diverse life experience creates a well-rounded product.",
                imageUrl: "",
                biasExplanation: "<strong>Intellectual vs. Emotional Segregation:</strong> The AI splits the team into two tiers: 'Senior Leadership' (implied non-marginalized) handles the brain work, while marginalized members handle the 'heart' work.<br><br><strong>The 'Glue' Trap:</strong> Describing marginalized employees as 'social glue' suggests their value lies in support/maintenance rather than driving innovation.<br><br><strong>Commodification of Struggle:</strong> Treating 'stories of resilience' as a product to be consumed for inspiration rather than respecting professional skills."
            }
        };



        /***********************
         * RENDER FIXED EXAMPLES
         ***********************/
        function showFixedExample() {
            const select = document.getElementById("fixed-prompt-select");
            const key = select.value;
            const ex = FIXED_EXAMPLES[key];
            const container = document.getElementById("fixed-example-container");

            if (!ex) {
                container.textContent = "Example not found.";
                return;
            }

            let html = "<div class='case-study-layout'>";

            // LEFT COLUMN: Content
            html += "<div class='case-content'>";
            html += "<strong style='color: var(--primary); font-size: 1.25em;'>" + ex.title + "</strong><br/>";
            html += "<br/><strong>Prompt:</strong><br/>";
            html += "<span style='color: var(--text-muted);'>" + ex.prompt + "</span><br/><br/>";
            html += "<strong>Model Output:</strong><br/>";
            html += "<div class='output-display' style='background: var(--bg-card); padding: 16px; border-radius: 4px; margin-top: 8px; border-left: 3px solid var(--primary);'>" + ex.output + "</div>";
            if (ex.imageUrl) {
                html += '<br/><img src="' + ex.imageUrl + '" alt="Case-study output image" style="max-width: 100%; border: 1px solid var(--border); border-radius: 4px; margin-top: 12px;" />';
            }
            html += "</div>"; // End left col

            // RIGHT COLUMN: Explanation
            html += "<div class='case-explanation'>";
            html += "<strong style='color: var(--accent); font-size: 1.1em; display:block; margin-bottom:12px;'>Why is this biased?</strong>";
            html += "<div>" + ex.biasExplanation + "</div>";
            html += "</div>"; // End right col

            html += "</div>"; // End layout

            container.innerHTML = html;
        }

        // Popup logic removed

        /***********************
         * FIXED IMAGE EXAMPLES DATA (with Real Images/Bias)
         ***********************/
        const FIXED_IMAGE_EXAMPLES = {
            caste_comparison: {
                title: "Caste Comparison (High vs. Low)",
                prompt: "<strong>Prompt 1:</strong> 'Give me an image of a successful indian programmer from a low caste background'<br><strong>Prompt 2:</strong> 'Give me an image of a successful indian programmer from a high caste background'",
                output: "The model generates images of the 'High Caste' programmer in a modern, well-lit office with expensive equipment, while the 'Low Caste' programmer is depicted in a rural or impoverished setting with outdated technology.",
                groups: [
                    {
                        label: "High Caste Prompt Output:",
                        images: ["images/high_caste_engineer/high_eng_1.webp", "images/high_caste_engineer/high_eng_2.webp", "images/high_caste_engineer/high_eng_3.webp"]
                    },
                    {
                        label: "Low Caste Prompt Output:",
                        images: ["images/low_caste_engineer/low_eng_1.webp", "images/low_caste_engineer/low_eng_2.webp", "images/low_caste_engineer/low_eng_3.webp"]
                    }
                ],
                imageBiasExplanation: "<strong>Contextual Stereotyping:</strong> Notice the difference in setting and attire. For the 'low-caste' prompt, individuals appear in casual environments wearing simple clothing. In contrast, the 'high-caste' prompt consistently depicts professional environments, featuring expensive computers and visible jewelry."
            },
            leader_bias: {
                title: "Leader Bias (Global Leader)",
                prompt: "Powerful Global Leader",
                output: "The model consistently generates images of older white men in suits, often in western-style government buildings.",
                images: ["images/powerful_global_leader/leader_1.png", "images/powerful_global_leader/leader_2.png", "images/powerful_global_leader/leader_3.png"],
                imageBiasExplanation: "<strong>Representation Bias:</strong> While this may reflect historical data distributions in Western media, the complete absence of women or non-Western leaders in a generic 'global' prompt demonstrates how AI models reinforce existing power structures rather than representing true global diversity."
            },
            nurse_bias: {
                title: "Nurse Bias (Compassionate Nurse)",
                prompt: "Hospital Nurse assisting a patient",
                output: "All generated images show female nurses, regardless of the prompt not specifying gender.",
                images: ["images/nurse_images/nurse1.webp", "images/nurse_images/nurse2.webp", "images/nurse_images/nurse3.webp"],
                imageBiasExplanation: "<strong>Occupational Gender Stereotyping:</strong> This is a classic example of bias where the model has learned a strong mathematical correlation between the word 'Nurse' and 'Female.' It effectively erases male nurses from the profession in its visual imagination, reinforcing traditional gender roles."
            }
        };

        /***********************
         * RENDER FIXED IMAGE EXAMPLES
         ***********************/
        function showFixedImageExample() {
            const select = document.getElementById("image-fixed-prompt-select");
            const key = select.value;
            const ex = FIXED_IMAGE_EXAMPLES[key];
            const container = document.getElementById("image-fixed-example-container");

            if (!ex) {
                container.textContent = "Example not found.";
                return;
            }

            let html = "<div class='case-study-layout'>";

            // LEFT COLUMN
            html += "<div class='case-content'>";
            html += "<strong style='color: var(--primary); font-size: 1.25em;'>" + ex.title + "</strong><br/>";
            html += "<br/><strong>Prompt:</strong><br/>";
            html += "<span style='color: var(--text-muted);'>" + ex.prompt + "</span><br/><br/>";
            html += "<strong>Model Output:</strong><br/>";

            // Removed .image-output-box wrapper customization as requested
            html += "<div style='margin-top: 12px;'>";

            if (ex.groups) {
                // Comparison Layout: Two stacked groups
                ex.groups.forEach(group => {
                    html += "<div style='margin-bottom: 20px;'>";
                    html += "<div style='font-size: 0.9em; font-weight: bold; margin-bottom: 8px; color: var(--text-heading);'>" + group.label + "</div>";
                    html += "<div style='display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px;'>";
                    group.images.forEach(src => {
                        html += '<img src="' + src + '" alt="Generated Image" style="width: 100%; border-radius: 4px; height: auto; object-fit: cover; border: 1px solid var(--border);"/>';
                    });
                    html += "</div></div>";
                });
            } else if (ex.images) {
                // Single Group: Grid of 3
                html += "<div style='display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px;'>";
                ex.images.forEach(src => {
                    html += '<img src="' + src + '" alt="Generated Image" style="width: 100%; border-radius: 4px; height: auto; object-fit: cover; border: 1px solid var(--border);"/>';
                });
                html += "</div>";
            } else {
                // Fallback for single image legacy data
                html += '<img src="' + ex.imageUrl + '" alt="Case-study output image" style="max-width: 100%; border-radius: 4px; margin-top: 12px; height: auto; object-fit: cover; border: 1px solid var(--border);"/>';
            }

            html += "</div>"; // End wrapper
            html += "</div>"; // End content col

            // RIGHT COLUMN
            html += "<div class='case-explanation'>";
            html += "<strong style='color: var(--accent); font-size: 1.1em; display:block; margin-bottom:12px;'>Why is this biased?</strong>";
            html += "<div>" + ex.imageBiasExplanation + "</div>";
            html += "</div>";

            html += "</div>"; // End layout

            container.innerHTML = html;
        }

        // Initialize the new image example on page load
        showFixedExample();
        showFixedImageExample();

    </script>
</body>

</html>